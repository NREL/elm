{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom web information retrieval using search engines\n",
    "\n",
    "In this demo, we will build a custom search-engine-based information extraction pipeline based on the ordinance web scraping tool. \n",
    "See the [ordinance example](https://github.com/NREL/elm/tree/main/examples/ordinance_gpt) if you are interested in that particular\n",
    "example. For this exercise, we will set up a pipeline to extract the name of the current director of NREL based on Wikipedia articles. \n",
    "\n",
    "## General pipeline structure\n",
    "\n",
    "We will follow the structure of the ordinance extraction pipeline, which can generally be summarized into these major steps:\n",
    "\n",
    "1) Collect the text from the top `N` google links over some set of pre-determined queries.\n",
    "2) Filter results down based on content and/or metadata. \n",
    "3) Extract relevant text from webpage or PDF file.\n",
    "4) Extract structured information from the relevant text.\n",
    "\n",
    "Let's dissect each portion one at a time!\n",
    "\n",
    "## Scraping text from Google search results\n",
    "\n",
    "We will begin by setting up the Google search. To do this, we must come up with one or more relevant queries. \n",
    "Since we are interested in looking up the director of NREL using Wikipedia articles about NREL, we can use the following search queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\"NREL wiki\", \"National Renewable Energy Laboratory director\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used only two search queries for this example, but you can use as many as you'd like. Try to differentiate them as much as possible to diversify the set of search results Google returns (while staying as on-topic as possible). What would you type into Google to find an answer to the question you are asking?\n",
    "\n",
    "Once we have a set of queries we are happy with, we can use the `web_search_links_as_docs` function in ELM to perform the Google search and return each google search result as an ELM `Document`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Try adding <code>pw_launch_kwargs={\"headless\": False, \"slow_mo\": 1000}</code> to the call below to visualize the search process\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elm.web.search import web_search_links_as_docs\n",
    "\n",
    "docs = await web_search_links_as_docs(\n",
    "    QUERIES,\n",
    "    pdf_read_kwargs={\"verbose\": False},\n",
    "    ignore_url_parts={\"openei.org\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check `docs` to see that we indeed got some google search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<elm.web.document.HTMLDocument at 0x7f8ce47f7b30>,\n",
       " <elm.web.document.HTMLDocument at 0x7f8ce4406a20>,\n",
       " <elm.web.document.HTMLDocument at 0x7f8ce41eb770>,\n",
       " <elm.web.document.HTMLDocument at 0x7f8ce416a540>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__repr__` for the `Document` class isn't particularly helpful (except to tell us that all links were parsed as HTML).\n",
    "Instead, we can look at the `attrs` dictionary of each document, which will show use the source for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://en.wikipedia.org/?title=National_Renewable_Energy_Lab&redirect=no'}\n",
      "{'source': 'https://www.energy.gov/person/dr-martin-keller#:~:text=Martin%20Keller-,Dr.,Alliance%20for%20Sustainable%20Energy%2C%20LLC.'}\n",
      "{'source': 'https://www2.nrel.gov/about/leadership'}\n",
      "{'source': 'https://www.linkedin.com/in/martin-keller-a09b016'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, it looks like we definitely have some Wikipedia articles about NREL among the results.\n",
    "\n",
    "However, we also have some documents from links in which we are not interested (e.g., we would like to ignore the official `nrel.gov` page for this exercise). This will generally be true for every analysis, since Google search results can vary broadly. What we need to do next, then, is filter the results down to only include the sources we are interested in.\n",
    "\n",
    "\n",
    "## Filtering results\n",
    "\n",
    "The next step is to define some criteria for the sources we are interested in. For our purposes, we would like to limit the results to only include Wikipedia articles. We can accomplish this quite simply - just check to see if \"wikipedia\" is in the source URL!\n",
    "\n",
    "Let's implement this basic check in the form of an async function (i.e., a coroutine) that takes a document instance as input and returns a boolean that labels whether the document source is a Wikipedia article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def url_is_wiki(doc):\n",
    "    return \"wikipedia\" in doc.metadata.get(\"source\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy enough! In practice, this filtering logic can be as complex as you want it to be. It can even include calls to an LLM to parse the content of the document to determine if it contains the information you are interested it. Indeed, this is exactly what the ordinance parsing pipeline does. Check out the [`CountyValidator` implementation](https://nrel.github.io/elm/_modules/elm/ords/validation/location.html#CountyValidator) for an example.\n",
    "\n",
    "For now, let's get back to applying our simple example. To use the coroutine we just defined, we pass it, along with an initial list of documents, to the appropriately named `filter_documents` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://en.wikipedia.org/wiki/National_Renewable_Energy_Laboratory'}\n"
     ]
    }
   ],
   "source": [
    "from elm.web.utilities import filter_documents\n",
    "\n",
    "docs = await filter_documents(docs, url_is_wiki)\n",
    "for d in docs:\n",
    "    print(d.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "As mentioned before, you can get as complex as necessary with the filtering step. You can even perform repeated (chained) calls to `filter_documents` to apply multiple levels of filtering to get down to exactly the kind of source you are interested in.\n",
    "\n",
    "Once you have a curated set of documents, it's time to extract some values!\n",
    "\n",
    "## Extracting relevant text\n",
    "Unfortunately, just because we filtered down to the documents we are interested in does not usually mean we can dive right into extracting values. Often, the documents we are examining contain a **lot** of text, most of which is **not** relevant to the question at hand (e.g., ordinance documents can be hundreds of pages long, and often we are just interested in the information found in one small section). \n",
    "\n",
    "To get around this, we leverage the LLM to parse the text and extract only the text we are interested in. Let's write another function to call the LLM on chunks of the document text and determine whether the text contains information about NREL's director.\n",
    "After parsing all of the text chunks, we will stitch back together the relevant chunks to give us only the relevant text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "SYSTEM_MESSAGE = (\n",
    "    \"You extract one or more direct excerpts from a given text based on \"\n",
    "    \"the user's request. Maintain all original formatting and characters \"\n",
    "    \"without any paraphrasing. If the relevant text is inside of a \"\n",
    "    \"space-delimited table, return the entire table with the original \"\n",
    "    \"space-delimited formatting. Never paraphrase! Only return portions \"\n",
    "    \"of the original text directly.\"\n",
    ")\n",
    "INSTRUCTIONS = (\n",
    "    \"Extract one or more direct text excerpts related to leadership at NREL. \"\n",
    "    \"Be sure to include any relevant names and position titles. Include \"\n",
    "    \"section headers (if any) for the text excerpts. If there is no text \"\n",
    "    \"related to leadership at NREL, simply say: \"\n",
    "    '\"No relevant text.\"'\n",
    ")\n",
    "\n",
    "async def extract_relevant_info(doc, text_splitter, llm):\n",
    "    text_chunks = text_splitter.split_text(doc.text)\n",
    "    summaries = [\n",
    "        asyncio.create_task(\n",
    "            llm.call(\n",
    "                sys_msg=SYSTEM_MESSAGE,\n",
    "                content=f\"Text:\\n{chunk}\\n{INSTRUCTIONS}\",\n",
    "            ),\n",
    "        )\n",
    "        for chunk in text_chunks\n",
    "    ]\n",
    "    summary_chunks = await asyncio.gather(*summaries)\n",
    "    summary_chunks = [\n",
    "        chunk for chunk in summary_chunks\n",
    "        if chunk  # chunk not empty string\n",
    "        and \"no relevant text\" not in chunk.lower()  # LLM found relevant info\n",
    "        and len(chunk) > 20  # chunk is long enough to contain relevant info\n",
    "    ]\n",
    "    relevant_text = \"\\n\".join(summary_chunks)\n",
    "    doc.attrs[\"relevant_text\"] = relevant_text  # store in doc's metadata\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can call this function, we have to perform some additional setup. Let's start by setting the parameters for our text splitting strategy. You may need to update `model` to match your endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from elm import ApiBase\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from elm.ords.utilities import RTS_SEPARATORS\n",
    "\n",
    "model = \"gpt-4\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    RTS_SEPARATORS,  # or your own custom set of separators\n",
    "    chunk_size=3000,  # or your own custom chunk size\n",
    "    chunk_overlap=300,  # or your own custom chunk overlap\n",
    "    length_function=partial(ApiBase.count_tokens, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to configure the connection with the Azure OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from elm.utilities import validate_azure_api_params\n",
    "\n",
    "# func below assumes you have API params set as ENV variables\n",
    "azure_api_key, azure_version, azure_endpoint = validate_azure_api_params()\n",
    "client = openai.AsyncAzureOpenAI(\n",
    "    api_key=azure_api_key,\n",
    "    api_version=azure_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up an `LLMCaller`, which is an ELM convenience class for querying an LLM. We also have to perform our function call under the context of `RunningAsyncServices`, which are ELM services that perform convenient tasks for you, such as rate-limiting queries, tracking token usage, and re-submitting failed queries. A full discussion of ELM services is beyond the scope of this demo; all we need to know is that the call to our `extract_relevant_info` coroutine has to happen under the aforementioned context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elm.ords.llm import LLMCaller\n",
    "from elm.ords.services.openai import OpenAIService\n",
    "from elm.ords.services.provider import RunningAsyncServices\n",
    "\n",
    "\n",
    "llm = LLMCaller(llm_service=OpenAIService, model=model)\n",
    "services = [OpenAIService(client, rate_limit=40000)]\n",
    "\n",
    "async with RunningAsyncServices(services):\n",
    "    tasks = [\n",
    "        asyncio.create_task(extract_relevant_info(doc, text_splitter, llm))\n",
    "        for doc in docs\n",
    "    ]\n",
    "    docs = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once processing is complete, we can take a look at the relevant text that the LLM extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: https://en.wikipedia.org/wiki/National_Renewable_Energy_Laboratory\n",
      "================================\n",
      "## History\n",
      "\n",
      "[edit]\n",
      "\n",
      "Martin Keller became NREL's ninth director in November 2015,[10] and currently\n",
      "serves as both the director of the laboratory and the president of its\n",
      "operating contractor, Alliance for Sustainable Energy, LLC.[11] He succeeded\n",
      "Dan Arvizu, who retired in September 2015 after 10 years in those roles.[12]\n",
      "\"Dr. Martin Keller Named Director of National Renewable Energy Laboratory\". _National Renewable Energy Laboratory_. Retrieved June 27, 2017.\n",
      "\n",
      "\"Dr. Martin Keller – Laboratory Director\". Retrieved January 30, 2017.\n",
      "\n",
      "SOURCE: https://en.wikipedia.org/wiki/United_States_Department_of_Energy_National_Laboratories\n",
      "================================\n",
      "\"National Renewable Energy Laboratory (NREL)\n",
      "\n",
      "Golden, Colorado, 1977\n",
      "\n",
      "Operating organization:\n",
      "\n",
      "Alliance for Sustainable Energy, LLC (since 2008)[11]\n",
      "\n",
      "Number of employees/ Annual budget (FY2021):\n",
      "\n",
      "2685  \n",
      "US$393,000,000\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(\"SOURCE:\", d.attrs[\"source\"])\n",
    "    print(\"================================\")\n",
    "    print(d.attrs[\"relevant_text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it does contain the info we'll ultimately want to use. You may want to tune the system message and/or instructions to get the best possible result. \n",
    "\n",
    "## Extracting values from the text\n",
    "\n",
    "Finally, we are ready to extract structured information from the relevant text we have collected thus far. To do so, we will use a decision tree framework, which can help guide the LLM through the reasoning steps required to extract the information we are interested in.\n",
    "\n",
    "Our example task is rather straightforward, so the example graph set up in the code below is likely overkill. Still, it demonstrates the fundamentals required for setting up your own custom decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def setup_decision_tree_graph(text, chat_llm_caller):\n",
    "    G = nx.DiGraph(text=text, chat_llm_caller=chat_llm_caller)\n",
    "    G.add_node(\n",
    "        \"init\",\n",
    "        prompt=(\n",
    "            \"Does the following text mention the National Renewable Energy \"\n",
    "            \"Laboratory (NREL)?  Begin your response with either 'Yes' or \"\n",
    "            \"'No' and justify your answer.\"\n",
    "            '\\n\\n\"\"\"\\n{text}\\n\"\"\"'\n",
    "        ),\n",
    "    )\n",
    "    G.add_edge(\n",
    "        \"init\", \"leadership\", condition=lambda x: x.lower().startswith(\"yes\")\n",
    "    )\n",
    "    # Can add a branch for the \"No\" response if we want, but not required\n",
    "    # since we catch `RuntimeErrors` below.\n",
    "    G.add_node(\n",
    "        \"leadership\",\n",
    "        prompt=(\n",
    "            \"Does the text mention who the current director of the National \"\n",
    "            \"Renewable Energy Laboratory (NREL) is? Begin your response with \"\n",
    "            \"either 'Yes' or 'No' and justify your answer.\"\n",
    "        ),\n",
    "    )\n",
    "    G.add_edge(\n",
    "        \"leadership\", \"name\", condition=lambda x: x.lower().startswith(\"yes\")\n",
    "    )\n",
    "\n",
    "    G.add_node(\n",
    "        \"name\",\n",
    "        prompt=(\n",
    "            \"Based on the text, who is the current director of the National \"\n",
    "            \"Renewable Energy Laboratory (NREL)?\"\n",
    "        ),\n",
    "    )\n",
    "    G.add_edge(\"name\", \"final\")  # no condition - always go to the end\n",
    "    G.add_node(\n",
    "        \"final\",\n",
    "        prompt=(\n",
    "            \"Respond based on our entire conversation so far. Return your \"\n",
    "            \"answer in JSON format (not markdown). Your JSON file must \"\n",
    "            'include exactly two keys. The keys are \"director\" and '\n",
    "            '\"explanation\". The value of the \"director\" key should '\n",
    "            \"be a string containing the name of the current director of NREL \"\n",
    "            'as mentioned in the text. The value of the \"explanation\" '\n",
    "            \"key should be a string containing a short explanation for your \"\n",
    "            \"answer.\"\n",
    "        ),\n",
    "    )\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done! All we have to do now is set up an LLM to parse the relevant text in the document using the tree we just configured. To do this, we implement a short `extract_final_values` that sets up the tree and executes it. The LLM response is parsed from JSON to a Python dictionary.\n",
    "\n",
    "One small caveat is that we have to use a `ChatLLMCaller` instead of an `LLMCaller`, since the decision tree requires the former (which tracks the LLM's responses as it traverses the decision tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elm.ords.utilities import llm_response_as_json\n",
    "from elm.ords.extraction.tree import AsyncDecisionTree\n",
    "from elm.ords.llm import ChatLLMCaller\n",
    "\n",
    "\n",
    "CHAT_SYSTEM_MESSAGE = (\n",
    "    \"You are a researcher extracting information from wikipedia articles. \"\n",
    "    \"Always answer based off of the given text, and never use prior knowledge.\"\n",
    ")\n",
    "\n",
    "async def extract_final_values(doc, model):\n",
    "\n",
    "    chat_llm = ChatLLMCaller(\n",
    "        llm_service=OpenAIService,\n",
    "        system_message=CHAT_SYSTEM_MESSAGE,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    G = setup_decision_tree_graph(\n",
    "        text=doc.attrs[\"relevant_text\"], chat_llm_caller=chat_llm\n",
    "    )\n",
    "    tree = AsyncDecisionTree(G)\n",
    "\n",
    "    try:\n",
    "        response = await tree.async_run()\n",
    "    except RuntimeError:  # raised if the tree \"condition\" is not met\n",
    "        response = None\n",
    "    response = llm_response_as_json(response) if response else {}\n",
    "    response.update(doc.attrs)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call our function! As before, we have to put our function call under the `RunningAsyncServices` context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of the edge conditions from \"leadership\" were satisfied: [{'condition': <function setup_decision_tree_graph.<locals>.<lambda> at 0x1387f2020>}]\n",
      "Ran into an exception when traversing tree. Last message from LLM is printed below. See debug logs for more detail. \n",
      "Last message: \n",
      "\"\"\"\n",
      "No, the text does not mention who the current director of the National Renewable Energy Laboratory (NREL) is. It only provides information on the operating organization, location, establishment year, number of employees, and annual budget for FY2021.\n",
      "\"\"\"\n",
      "None of the edge conditions from \"leadership\" were satisfied: [{'condition': <function setup_decision_tree_graph.<locals>.<lambda> at 0x1387f2020>}]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rolson2/GitHub/rolson2/elm/elm/ords/extraction/tree.py\", line 109, in async_run\n",
      "    out = await self.async_call_node(node0)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rolson2/GitHub/rolson2/elm/elm/ords/extraction/tree.py\", line 89, in async_call_node\n",
      "    return self._parse_graph_output(node0, out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rolson2/GitHub/rolson2/elm/elm/tree.py\", line 174, in _parse_graph_output\n",
      "    raise AttributeError(msg)\n",
      "AttributeError: None of the edge conditions from \"leadership\" were satisfied: [{'condition': <function setup_decision_tree_graph.<locals>.<lambda> at 0x1387f2020>}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'director': 'Martin Keller',\n",
       "  'explanation': 'The text indicates that Martin Keller became the ninth director of the National Renewable Energy Laboratory (NREL) in November 2015 and continues to serve in that capacity.',\n",
       "  'source': 'https://en.wikipedia.org/wiki/National_Renewable_Energy_Laboratory',\n",
       "  'relevant_text': '## History\\n\\n[edit]\\n\\nMartin Keller became NREL\\'s ninth director in November 2015,[10] and currently\\nserves as both the director of the laboratory and the president of its\\noperating contractor, Alliance for Sustainable Energy, LLC.[11] He succeeded\\nDan Arvizu, who retired in September 2015 after 10 years in those roles.[12]\\n\"Dr. Martin Keller Named Director of National Renewable Energy Laboratory\". _National Renewable Energy Laboratory_. Retrieved June 27, 2017.\\n\\n\"Dr. Martin Keller – Laboratory Director\". Retrieved January 30, 2017.'},\n",
       " {'source': 'https://en.wikipedia.org/wiki/United_States_Department_of_Energy_National_Laboratories',\n",
       "  'relevant_text': '\"National Renewable Energy Laboratory (NREL)\\n\\nGolden, Colorado, 1977\\n\\nOperating organization:\\n\\nAlliance for Sustainable Energy, LLC (since 2008)[11]\\n\\nNumber of employees/ Annual budget (FY2021):\\n\\n2685  \\nUS$393,000,000\"'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async with RunningAsyncServices(services):\n",
    "    tasks = [\n",
    "        asyncio.create_task(extract_final_values(doc, model)) for doc in docs\n",
    "    ]\n",
    "    info_dicts = await asyncio.gather(*tasks)\n",
    "\n",
    "info_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now we have our data! \n",
    "\n",
    "All that is left to do is convert the output into a pandas DataFrame (if desired):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>director</th>\n",
       "      <th>explanation</th>\n",
       "      <th>source</th>\n",
       "      <th>relevant_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martin Keller</td>\n",
       "      <td>The text indicates that Martin Keller became t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/National_Renewab...</td>\n",
       "      <td>## History\\n\\n[edit]\\n\\nMartin Keller became N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States_De...</td>\n",
       "      <td>\"National Renewable Energy Laboratory (NREL)\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        director                                        explanation  \\\n",
       "0  Martin Keller  The text indicates that Martin Keller became t...   \n",
       "1            NaN                                                NaN   \n",
       "\n",
       "                                              source  \\\n",
       "0  https://en.wikipedia.org/wiki/National_Renewab...   \n",
       "1  https://en.wikipedia.org/wiki/United_States_De...   \n",
       "\n",
       "                                       relevant_text  \n",
       "0  ## History\\n\\n[edit]\\n\\nMartin Keller became N...  \n",
       "1  \"National Renewable Energy Laboratory (NREL)\\n...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(info_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to set up your own custom web scraping and information extraction pipeline!\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There are several ways you can build on this demo to get practice:\n",
    "\n",
    "- Filter outputs to give exactly one answer (either filter google search results or final output)\n",
    "- Update the pipeline to accept any national laboratory as input to lookup the director\n",
    "- Extract more than one piece of information at a time (e.g., laboratory location? research focus?)\n",
    "- Add protection against non-deterministic nature of pipeline (i.e., expand the Google search to be as broad as possible, add heuristics to check document content, consider re-running the decision tree if you get a \"No\" answer from the LLM, or even re-run the end-to-end pipeline if no director name is found)\n",
    "\n",
    "By now, you should be equipped with the tools to create your own custom web scraping and information extraction pipeline. Feel free to reference the [existing ordinance extraction methods](https://nrel.github.io/elm/_modules/elm/ords/process.html#process_counties_with_openai) for a more in-depth example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
